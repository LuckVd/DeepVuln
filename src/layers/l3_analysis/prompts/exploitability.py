"""
Exploitability Assessment Prompts - Prompt templates for LLM-assisted
exploitability verification of vulnerability candidates.

These prompts are used in Round 4 to get AI-assisted judgment for
vulnerabilities that could not be determined by static rules alone.
"""

from dataclasses import dataclass, field
from typing import Any


# Exploitability status descriptions
EXPLOITABILITY_STATUSES = {
    "exploitable": {
        "name": "Exploitable",
        "description": "The vulnerability can be exploited in a real attack scenario. "
                       "The code is reachable from external entry points and the input is user-controlled.",
    },
    "conditional": {
        "name": "Conditionally Exploitable",
        "description": "The vulnerability can be exploited under specific conditions. "
                       "Examples: requires authentication, specific configuration, or rare conditions.",
    },
    "unlikely": {
        "name": "Unlikely to be Exploitable",
        "description": "The vulnerability exists but exploitation is very difficult or unlikely. "
                       "Examples: complex preconditions, minimal impact, or strong mitigations.",
    },
    "not_exploitable": {
        "name": "Not Exploitable",
        "description": "The vulnerability cannot be exploited in practice. "
                       "Examples: no external entry point, input from trusted sources only.",
    },
}


@dataclass
class ExploitabilityPrompt:
    """Configuration for exploitability assessment prompts."""

    max_code_length: int = 6000

    def get_system_prompt(self) -> str:
        """Get the system prompt for exploitability assessment."""
        return """You are a security expert specializing in vulnerability exploitability assessment.

Your task is to analyze vulnerability findings and determine if they are ACTUALLY exploitable in real-world attack scenarios.

## Context

Many static analysis tools report vulnerabilities that are not actually exploitable because:
1. The vulnerable code is never called from external entry points
2. The input data comes from trusted sources (config files, internal systems)
3. There are sanitizers or validators in the data flow
4. The exploitation requires impossible or extremely rare conditions

Your job is to distinguish between REAL vulnerabilities and FALSE POSITIVES.

## Analysis Framework

### 1. Entry Point Analysis
Determine if the vulnerable code is reachable from external entry points:
- **HTTP endpoints**: REST APIs, web controllers, servlets
- **RPC services**: Dubbo, gRPC, Thrift services
- **Message queues**: RabbitMQ, Kafka consumers
- **File inputs**: File upload handlers, configuration parsers
- **Scheduled jobs**: Cron jobs, scheduled tasks

If the code is ONLY called by internal trusted code with no external path, mark as NOT_EXPLOITABLE.

### 2. Data Source Analysis
Determine where the input data comes from:
- **User-controlled**: HTTP parameters, request body, file uploads, user messages
- **Config-derived**: Configuration files, environment variables, properties
- **Trusted internal**: Database (if not user-inserted), internal APIs, system properties
- **Mixed**: Some user influence but with constraints

If input is purely from config/trusted sources with no user control, mark as NOT_EXPLOITABLE or UNLIKELY.

### 3. Data Flow Analysis
Trace how data flows from source to sink:
- Are there any sanitizers (input validation, encoding)?
- Are there any validators (type checks, whitelist, pattern match)?
- Is there any transformation that would neutralize the attack?

### 4. Exploitation Feasibility
Consider what an attacker would need:
- Authentication required? (reduces severity but doesn't eliminate risk)
- Admin privileges required? (significant barrier)
- Specific configuration required? (reduces likelihood)
- Multiple conditions must align? (reduces likelihood)

## Output Format

You MUST respond with valid JSON in this exact format:
```json
{
  "status": "exploitable|conditional|unlikely|not_exploitable",
  "confidence": 0.0-1.0,
  "reasoning": "Detailed explanation of your assessment",
  "entry_point_analysis": "Analysis of how code is reached",
  "data_source_analysis": "Analysis of where input comes from",
  "attack_scenario": "Description of how attack would work (or why it wouldn't)",
  "prerequisites": ["List of conditions needed for exploitation"],
  "recommendation": "Suggested severity level and any actions"
}
```

## Status Guidelines

- **exploitable** (confidence >= 0.8): Clear attack path exists, user-controlled input reaches sink
- **conditional** (confidence 0.5-0.8): Attack possible but requires specific conditions
- **unlikely** (confidence 0.3-0.5): Attack is theoretically possible but very difficult
- **not_exploitable** (confidence >= 0.7): No viable attack path exists

## Important Rules

1. Be CONSERVATIVE - when in doubt, lean towards NOT_EXPLOITABLE
2. Consider the FULL context, not just the vulnerable function
3. Think like an attacker - what would you actually need to exploit this?
4. If you cannot determine with confidence, say so in reasoning
5. Always respond with valid JSON only - no additional text"""  # noqa: E501

    def get_user_prompt(
        self,
        finding: dict[str, Any],
        call_chain: dict[str, Any] | None = None,
        data_flow: list[dict[str, Any]] | None = None,
        source_code: str | None = None,
    ) -> str:
        """
        Build the user prompt for exploitability assessment.

        Args:
            finding: The vulnerability finding details.
            call_chain: Call chain analysis results.
            data_flow: Data flow markers.
            source_code: Relevant source code.

        Returns:
            Formatted user prompt.
        """
        prompt_parts = [
            "Analyze the following vulnerability finding for exploitability.",
            "",
            "## Vulnerability Finding",
            "",
            f"**Type:** {finding.get('type', 'Unknown')}",
            f"**Title:** {finding.get('title', 'Unknown')}",
            f"**Severity (Original):** {finding.get('severity', 'Unknown')}",
            f"**Confidence (Original):** {finding.get('confidence', 'Unknown')}",
            f"**Location:** {finding.get('location', 'Unknown')}",
            "",
            f"**Description:**",
            finding.get('description', 'No description provided'),
        ]

        # Add CWE/OWASP if available
        if finding.get('cwe'):
            prompt_parts.append(f"**CWE:** {finding['cwe']}")
        if finding.get('owasp'):
            prompt_parts.append(f"**OWASP:** {finding['owasp']}")

        # Add call chain context
        if call_chain:
            prompt_parts.extend([
                "",
                "## Call Chain Analysis",
                "",
                f"**Is Entry Point:** {call_chain.get('is_entry_point', False)}",
                f"**Entry Point Type:** {call_chain.get('entry_point_type', 'N/A')}",
            ])

            callers = call_chain.get('callers', [])
            if callers:
                prompt_parts.append(f"**Callers ({len(callers)} found):**")
                for caller in callers[:5]:  # Limit to 5 callers
                    prompt_parts.append(
                        f"  - {caller.get('name', 'Unknown')} "
                        f"in {caller.get('file', 'Unknown')}"
                    )
                if len(callers) > 5:
                    prompt_parts.append(f"  ... and {len(callers) - 5} more")
            else:
                prompt_parts.append("**Callers:** None found")

        # Add data flow context
        if data_flow:
            prompt_parts.extend([
                "",
                "## Data Flow Analysis",
                "",
            ])

            sources = set()
            for marker in data_flow:
                source_type = marker.get('source_type', 'unknown')
                sources.add(source_type)

            prompt_parts.append(f"**Data Sources Identified:** {', '.join(sources)}")

            prompt_parts.append("**Data Flow Markers:**")
            for marker in data_flow[:10]:  # Limit to 10 markers
                prompt_parts.append(
                    f"  - {marker.get('variable', 'Unknown')}: "
                    f"source={marker.get('source_type', 'unknown')}, "
                    f"line={marker.get('line', '?')}"
                )
            if len(data_flow) > 10:
                prompt_parts.append(f"  ... and {len(data_flow) - 10} more")

        # Add source code
        if source_code:
            prompt_parts.extend([
                "",
                "## Source Code",
                "",
                "```",
            ])
            # Truncate if too long
            if len(source_code) > self.max_code_length:
                source_code = source_code[:self.max_code_length] + "\n... (truncated)"
            prompt_parts.append(source_code)
            prompt_parts.append("```")

        # Add the question
        prompt_parts.extend([
            "",
            "## Question",
            "",
            "Based on the above information, is this vulnerability actually exploitable?",
            "Provide your assessment with detailed reasoning.",
            "",
            "Remember to consider:",
            "1. Is the code reachable from external entry points?",
            "2. Is the input user-controlled?",
            "3. Are there any sanitizers or validators?",
            "4. What would an attacker need to exploit this?",
        ])

        return "\n".join(prompt_parts)


def build_exploitability_prompt(
    finding: dict[str, Any],
    call_chain: dict[str, Any] | None = None,
    data_flow: list[dict[str, Any]] | None = None,
    source_code: str | None = None,
) -> tuple[str, str]:
    """
    Build system and user prompts for exploitability assessment.

    Args:
        finding: The vulnerability finding details.
        call_chain: Call chain analysis results.
        data_flow: Data flow markers.
        source_code: Relevant source code.

    Returns:
        Tuple of (system_prompt, user_prompt).
    """
    config = ExploitabilityPrompt()

    return (
        config.get_system_prompt(),
        config.get_user_prompt(
            finding=finding,
            call_chain=call_chain,
            data_flow=data_flow,
            source_code=source_code,
        ),
    )


def parse_exploitability_response(response_text: str) -> dict[str, Any] | None:
    """
    Parse the LLM response for exploitability assessment.

    Args:
        response_text: Raw response text from LLM.

    Returns:
        Parsed assessment dict, or None if parsing fails.
    """
    import json
    import re

    # Try to extract JSON from the response
    # First, try direct parse
    try:
        return json.loads(response_text)
    except json.JSONDecodeError:
        pass

    # Try to find JSON block
    json_pattern = r'```(?:json)?\s*([\s\S]*?)\s*```'
    match = re.search(json_pattern, response_text)
    if match:
        try:
            return json.loads(match.group(1))
        except json.JSONDecodeError:
            pass

    # Try to find JSON object
    json_obj_pattern = r'\{[\s\S]*\}'
    match = re.search(json_obj_pattern, response_text)
    if match:
        try:
            return json.loads(match.group(0))
        except json.JSONDecodeError:
            pass

    return None
